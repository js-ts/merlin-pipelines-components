name: Transform dataset op
description: Component to transform a dataset according to the workflow definitions.
inputs:
- {name: workflow, type: Artifact}
- {name: parquet_dataset, type: Dataset}
- {name: n_workers, type: Integer}
- {name: shuffle, type: String, optional: true}
- {name: device_limit_frac, type: Float, default: '0.8', optional: true}
- {name: device_pool_frac, type: Float, default: '0.9', optional: true}
- {name: part_mem_frac, type: Float, default: '0.125', optional: true}
outputs:
- {name: transformed_dataset, type: Dataset}
implementation:
  container:
    image: nvcr.io/nvidia/merlin/merlin-training:21.09
    command:
    - sh
    - -ec
    - |
      program_path=$(mktemp -d)
      printf "%s" "$0" > "$program_path/ephemeral_component.py"
      python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
    - "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing\
      \ import *\n\ndef transform_dataset_op(\n    workflow: Input[Artifact],\n  \
      \  parquet_dataset: Input[Dataset],\n    transformed_dataset: Output[Dataset],\n\
      \    n_workers: int,\n    shuffle: str = None,\n    device_limit_frac: float\
      \ = 0.8,\n    device_pool_frac: float = 0.9,\n    part_mem_frac: float = 0.125,\n\
      ):\n    '''\n    Component to transform a dataset according to the workflow\
      \ definitions.\n\n    workflow: Input[Artifact]\n        Input metadata with\
      \ the path to the fitted_workflow\n    parquet_dataset: Input[Dataset]\n   \
      \     Location of the converted dataset in GCS and split name\n    transformed_dataset:\
      \ Output[Dataset]\n        Split name of the transformed dataset.\n    n_workers:\
      \ int\n        Number of GPUs allocated to do the transformation\n    '''\n\
      \    from preprocessing import etl\n    import logging\n    import os\n\n  \
      \  logging.basicConfig(level=logging.INFO)\n\n    # Create Dask cluster\n  \
      \  logging.info('Creating Dask cluster.')\n    client = etl.create_cluster(\n\
      \        n_workers=n_workers,\n        device_limit_frac=device_limit_frac,\
      \ \n        device_pool_frac=device_pool_frac\n    )\n\n    logging.info('Loading\
      \ workflow and statistics')\n    criteo_workflow = etl.load_workflow(\n    \
      \    workflow_path=workflow.path,\n        client=client\n    )\n\n    split\
      \ = parquet_dataset.metadata['split']\n\n    logging.info(f'Creating dataset\
      \ definition for {split} split')\n    dataset = etl.create_parquet_dataset(\n\
      \        client=client,\n        data_path=os.path.join(\n            parquet_dataset.path.replace('/gcs/',\
      \ 'gs://'),\n            split\n        ),\n        part_mem_frac=part_mem_frac\n\
      \    )\n\n    logging.info('Workflow is loaded')\n    logging.info('Starting\
      \ workflow transformation')\n    dataset = etl.transform_dataset(\n        dataset=dataset,\n\
      \        workflow=criteo_workflow\n    )\n\n    logging.info('Applying transformation')\n\
      \    etl.save_dataset(\n        dataset, os.path.join(transformed_dataset.path,\
      \ split)\n    )\n\n    transformed_dataset.metadata['split'] = split\n\n"
    args:
    - --executor_input
    - {executorInput: null}
    - --function_to_execute
    - transform_dataset_op
