name: Session based recsys
inputs:
- {name: Output_Model_Path, type: String, default: /dli/task/model_repository, optional: true}
- {name: Output_Model_Name, type: String, default: t4r_pytorch, optional: true}
- {name: Data_Input, type: String, default: /dli/task/data/, optional: true}
- {name: Data_Output, type: String, default: /dli/task/data/sessions_by_day, optional: true}
- {name: local_rank, type: Integer, default: '-1', optional: true}
- {name: training_args_output_dir, type: String, default: ./tmp, optional: true}
- {name: training_args_max_sequence_length, type: Integer, default: '20', optional: true}
- {name: training_args_data_loader_engine, type: String, default: nvtabular, optional: true}
- {name: training_args_num_train_epochs, type: Integer, default: '3', optional: true}
- name: training_args_dataloader_drop_last
  default: "False"
  optional: true
- {name: training_args_per_device_train_batch_size, type: Integer, default: '256',
  optional: true}
- {name: training_args_per_device_eval_batch_size, type: Integer, default: '32', optional: true}
- {name: training_args_gradient_accumulation_steps, type: Integer, default: '1', optional: true}
- {name: training_args_learning_rate, type: Integer, optional: true}
- {name: training_args_report_to, type: JsonArray, default: '[]', optional: true}
- {name: training_args_logging_steps, type: Integer, default: '200', optional: true}
outputs:
- {name: model, type: String}
implementation:
  container:
    image: nvcr.io/nvidia/merlin/merlin-pytorch-training:21.11
    command:
    - sh
    - -ec
    - |
      program_path=$(mktemp -d)
      printf "%s" "$0" > "$program_path/ephemeral_component.py"
      python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
    - "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing\
      \ import *\n\ndef session_based_recsys(    \n    Output_Model_Path: str=\"/dli/task/model_repository\"\
      ,\n    Output_Model_Name: str = \"t4r_pytorch\",\n    Data_Input: str=\"/dli/task/data/\"\
      ,\n    Data_Output: str = \"/dli/task/data/sessions_by_day\",\n    local_rank\
      \ : int= -1,\n    training_args_output_dir: str=\"./tmp\",\n    training_args_max_sequence_length:\
      \ int=20,\n    training_args_data_loader_engine: str='nvtabular',\n    training_args_num_train_epochs:\
      \ int=3, \n    training_args_dataloader_drop_last=False,\n    training_args_per_device_train_batch_size\
      \ : int= 256,\n    training_args_per_device_eval_batch_size : int= 32,\n   \
      \ training_args_gradient_accumulation_steps : int= 1,\n    training_args_learning_rate:\
      \ int=0.000666,\n    training_args_report_to :list= [],\n    training_args_logging_steps:\
      \ int=200,\n)->NamedTuple('Model',[('model',str)]):\n    # Training an RNN-based\
      \ Session-based Recommendation Model\n    import os\n    import glob\n\n   \
      \ import torch \n    import transformers4rec.torch as tr\n\n    from transformers4rec.torch.ranking_metric\
      \ import NDCGAt, RecallAt\n    from transformers4rec.torch.utils.examples_utils\
      \ import wipe_memory\n\n    # Instantiates Schema object from a `schema` file.\n\
      \n    from merlin_standard_lib import Schema\n    # Define schema object to\
      \ pass it to the TabularSequenceFeatures class\n    INPUT_DATA_DIR = Data_Input\n\
      \n    SCHEMA_PATH = os.path.join(INPUT_DATA_DIR, 'schema_tutorial.pb')\n   \
      \ schema = Schema().from_proto_text(SCHEMA_PATH)\n    schema = schema.select_by_name(['product_id-list_seq'])\n\
      \n    # Defining the input block: `TabularSequenceFeatures`\n\n    # Define\
      \ input block\n    sequence_length = 20\n    inputs = tr.TabularSequenceFeatures.from_schema(\n\
      \            schema,\n            max_sequence_length= sequence_length,\n  \
      \          masking = 'causal',\n        )\n\n    # Connecting the blocks with\
      \ `SequentialBlock`\n\n    d_model = 128\n    body = tr.SequentialBlock(\n \
      \           inputs,\n            tr.MLPBlock([d_model]),\n            tr.Block(torch.nn.GRU(input_size=d_model,\
      \ hidden_size=d_model, num_layers=1), [None, 20, d_model])\n    )\n\n    # Item\
      \ Prediction head and tying embeddings\n\n    head = tr.Head(\n    body,\n \
      \       tr.NextItemPredictionTask(weight_tying=True, hf_format=True, \n    \
      \                            metrics=[NDCGAt(top_ks=[10, 20], labels_onehot=True),\
      \  \n                                        RecallAt(top_ks=[10, 20], labels_onehot=True)]),\n\
      \    )\n    model = tr.Model(head)\n\n    # Define a Dataloader function from\
      \ schema\n\n    # import NVTabular dependencies\n    from transformers4rec.torch.utils.data_utils\
      \ import NVTabularDataLoader\n\n    x_cat_names, x_cont_names = ['product_id-list_seq'],\
      \ []\n\n    # dictionary representing max sequence length for column\n    sparse_features_max\
      \ = {\n        fname: sequence_length\n        for fname in x_cat_names + x_cont_names\n\
      \    }\n\n    # Define a `get_dataloader` function to call in the training loop\n\
      \    def get_dataloader(path, batch_size=32):\n\n        return NVTabularDataLoader.from_schema(\n\
      \            schema,\n            path, \n            batch_size,\n        \
      \    max_sequence_length=sequence_length,\n            sparse_names=x_cat_names\
      \ + x_cont_names,\n            sparse_max=sparse_features_max,\n    )\n\n  \
      \  # Daily Fine-Tuning: Training over a time window\n\n    from transformers4rec.config.trainer\
      \ import T4RecTrainingArguments\n    from transformers4rec.torch import Trainer\n\
      \n    #Set arguments for training \n    train_args = T4RecTrainingArguments(local_rank\
      \ = local_rank, \n                                        dataloader_drop_last\
      \ = training_args_dataloader_drop_last,\n                                  \
      \      report_to = training_args_report_to,   #set empy list to avoig logging\
      \ metrics to Weights&Biases\n                                        gradient_accumulation_steps\
      \ = training_args_gradient_accumulation_steps,\n                           \
      \             per_device_train_batch_size = training_args_per_device_train_batch_size,\
      \ \n                                        per_device_eval_batch_size = training_args_per_device_eval_batch_size,\n\
      \                                        output_dir = training_args_output_dir,\
      \ \n                                        max_sequence_length=sequence_length,\n\
      \                                        learning_rate=0.00071,\n          \
      \                              num_train_epochs=training_args_num_train_epochs,\n\
      \                                        logging_steps=training_args_logging_steps,\n\
      \                                    )\n\n    # Instantiate the T4Rec Trainer,\
      \ which manages training and evaluation\n    trainer = Trainer(\n        model=model,\n\
      \        args=train_args,\n        schema=schema,\n        compute_metrics=True,\n\
      \    )\n    #recieve input from the \n    OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\"\
      , Data_Output)\n\n    # os.system(\"%%time\")\n\n    start_time_window_index\
      \ = 1\n    final_time_window_index = 4\n    for time_index in range(start_time_window_index,\
      \ final_time_window_index):\n        # Set data \n        time_index_train =\
      \ time_index\n        time_index_eval = time_index + 1\n        train_paths\
      \ = glob.glob(os.path.join(OUTPUT_DIR, f\"{time_index_train}/train.parquet\"\
      ))\n        eval_paths = glob.glob(os.path.join(OUTPUT_DIR, f\"{time_index_eval}/valid.parquet\"\
      ))\n        # Train on day related to time_index \n        print('*'*20)\n \
      \       print(\"Launch training for day %s are:\" %time_index)\n        print('*'*20\
      \ + '\\n')\n        trainer.train_dataset_or_path = train_paths\n        trainer.reset_lr_scheduler()\n\
      \        trainer.train()\n        trainer.state.global_step +=1\n        # Evaluate\
      \ on the following day\n        trainer.eval_dataset_or_path = eval_paths\n\
      \        train_metrics = trainer.evaluate(metric_key_prefix='eval')\n      \
      \  print('*'*20)\n        print(\"Eval results for day %s are:\\t\" %time_index_eval)\n\
      \        print('\\n' + '*'*20 + '\\n')\n        for key in sorted(train_metrics.keys()):\n\
      \            print(\" %s = %s\" % (key, str(train_metrics[key]))) \n       \
      \ wipe_memory()\n\n        from collections import namedtuple\n        upload_file_path\
      \ = namedtuple('Output',['OutputPath_Name'])\n        return upload_file_path(model)\n\
      \n\n    # Training a Transformer-based Session-based Recommendation Model\n\n\
      \    import os\n    import glob\n\n    import torch \n    import transformers4rec.torch\
      \ as tr\n\n    from transformers4rec.torch.ranking_metric import NDCGAt, RecallAt\n\
      \    from transformers4rec.torch.utils.examples_utils import wipe_memory\n\n\
      \    # As we did above, we start with defining our schema object and filtering\
      \ only the `product_id` feature for training.\n    from merlin_standard_lib\
      \ import Schema\n    # Define schema object to pass it to the TabularSequenceFeatures\
      \ class\n    INPUT_DATA_DIR = Data_Input\n\n    SCHEMA_PATH = os.path.join(INPUT_DATA_DIR,\
      \ 'schema_tutorial.pb')\n    schema = Schema().from_proto_text(SCHEMA_PATH)\n\
      \    schema = schema.select_by_name(['product_id-list_seq'])\n\n    # Define\
      \ input block\n    sequence_length, d_model = 20, 192\n    # Define input module\
      \ to process tabular input-features and to prepare masked inputs\n    inputs=\
      \ tr.TabularSequenceFeatures.from_schema(\n        schema,\n        max_sequence_length=sequence_length,\n\
      \        d_output=d_model,\n        masking=\"mlm\",\n    )\n\n    # Define\
      \ XLNetConfig class and set default parameters for HF XLNet config  \n    transformer_config\
      \ = tr.XLNetConfig.build(\n        d_model=d_model, n_head=4, n_layer=2, total_seq_length=sequence_length\n\
      \    )\n    # Define the model block including: inputs, masking, projection\
      \ and transformer block.\n    body = tr.SequentialBlock(\n        inputs, tr.MLPBlock([192]),\
      \ tr.TransformerBlock(transformer_config, masking=inputs.masking)\n    )\n\n\
      \    # Define the head for to next item prediction task \n    head = tr.Head(\n\
      \        body,\n        tr.NextItemPredictionTask(weight_tying=True, hf_format=True,\
      \ \n                                metrics=[NDCGAt(top_ks=[10, 20], labels_onehot=True),\
      \  \n                                        RecallAt(top_ks=[10, 20], labels_onehot=True)]),\n\
      \    )\n\n    # Get the end-to-end Model class \n    model = tr.Model(head)\n\
      \n    from transformers4rec.config.trainer import T4RecTrainingArguments\n \
      \   from transformers4rec.torch import Trainer\n\n    #Set arguments for training\
      \ \n    training_args = T4RecTrainingArguments(\n                output_dir=training_args_output_dir,\n\
      \                max_sequence_length=training_args_max_sequence_length,\n  \
      \              data_loader_engine=training_args_data_loader_engine,\n      \
      \          num_train_epochs=training_args_num_train_epochs, \n             \
      \   dataloader_drop_last=training_args_dataloader_drop_last,\n             \
      \   per_device_train_batch_size = training_args_per_device_train_batch_size,\n\
      \                per_device_eval_batch_size = training_args_per_device_eval_batch_size,\n\
      \                gradient_accumulation_steps = training_args_gradient_accumulation_steps,\n\
      \                learning_rate=training_args_learning_rate,\n              \
      \  report_to = training_args_report_to,\n                logging_steps=training_args_logging_steps,\n\
      \            )\n\n    # Instantiate the T4Rec Trainer, which manages training\
      \ and evaluation\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n\
      \        schema=schema,\n        compute_metrics=True,\n    )\n\n    OUTPUT_DIR\
      \ = os.environ.get(\"OUTPUT_DIR\", Data_Output)\n\n    # os.system(\"%%time\"\
      )\n    start_time_window_index = 1\n    final_time_window_index = 4\n    for\
      \ time_index in range(start_time_window_index, final_time_window_index):\n \
      \       # Set data \n        time_index_train = time_index\n        time_index_eval\
      \ = time_index + 1\n        train_paths = glob.glob(os.path.join(OUTPUT_DIR,\
      \ f\"{time_index_train}/train.parquet\"))\n        eval_paths = glob.glob(os.path.join(OUTPUT_DIR,\
      \ f\"{time_index_eval}/valid.parquet\"))\n        # Train on day related to\
      \ time_index \n        print('*'*20)\n        print(\"Launch training for day\
      \ %s are:\" %time_index)\n        print('*'*20 + '\\n')\n        trainer.train_dataset_or_path\
      \ = train_paths\n        trainer.reset_lr_scheduler()\n        trainer.train()\n\
      \        trainer.state.global_step +=1\n        # Evaluate on the following\
      \ day\n        trainer.eval_dataset_or_path = eval_paths\n        train_metrics\
      \ = trainer.evaluate(metric_key_prefix='eval')\n        print('*'*20)\n    \
      \    print(\"Eval results for day %s are:\\t\" %time_index_eval)\n        print('\\\
      n' + '*'*20 + '\\n')\n        for key in sorted(train_metrics.keys()):\n   \
      \         print(\" %s = %s\" % (key, str(train_metrics[key]))) \n        wipe_memory()\n\
      \n        # Train XLNET with Side Information for Next Item Prediction\n\n \
      \   import os\n    import glob\n    import nvtabular as nvt\n\n    import torch\
      \ \n    import transformers4rec.torch as tr\n\n    from transformers4rec.torch.ranking_metric\
      \ import NDCGAt, RecallAt\n    from transformers4rec.torch.utils.examples_utils\
      \ import wipe_memory\n\n    # Define categorical and continuous columns to fed\
      \ to training model \n    # add it in parameters\n    x_cat_names = ['product_id-list_seq',\
      \ 'category_id-list_seq', 'brand-list_seq']\n    x_cont_names = ['product_recency_days_log_norm-list_seq',\
      \ 'et_dayofweek_sin-list_seq', 'et_dayofweek_cos-list_seq', \n             \
      \       'price_log_norm-list_seq', 'relative_price_to_avg_categ_id-list_seq']\n\
      \n\n    from merlin_standard_lib import Schema\n\n    # Define schema object\
      \ to pass it to the TabularSequenceFeatures class\n    INPUT_DATA_DIR = os.environ.get(\"\
      INPUT_DATA_DIR\", \"/dli/task/data/\")\n\n    SCHEMA_PATH = os.path.join(INPUT_DATA_DIR,\
      \ 'schema_tutorial.pb')\n    schema = Schema().from_proto_text(SCHEMA_PATH)\n\
      \    schema = schema.select_by_name(x_cat_names + x_cont_names)\n\n    # Define\
      \ input block\n    sequence_length, d_model = 20, 192\n    # Define input module\
      \ to process tabular input-features and to prepare masked inputs\n    inputs=\
      \ tr.TabularSequenceFeatures.from_schema(\n        schema,\n        max_sequence_length=sequence_length,\n\
      \        aggregation=\"concat\",\n        d_output=d_model,\n        masking=\"\
      mlm\",\n    )\n\n    # Define XLNetConfig class and set default parameters for\
      \ HF XLNet config  \n    transformer_config = tr.XLNetConfig.build(\n      \
      \  d_model=d_model, n_head=4, n_layer=2, total_seq_length=sequence_length\n\
      \    )\n    # Define the model block including: inputs, masking, projection\
      \ and transformer block.\n    body = tr.SequentialBlock(\n        inputs, tr.MLPBlock([192]),\
      \ tr.TransformerBlock(transformer_config, masking=inputs.masking)\n    )\n\n\
      \    # Define the head related to next item prediction task \n    head = tr.Head(\n\
      \        body,\n        tr.NextItemPredictionTask(weight_tying=True, hf_format=True,\
      \ \n                                        metrics=[NDCGAt(top_ks=[10, 20],\
      \ labels_onehot=True),  \n                                                RecallAt(top_ks=[10,\
      \ 20], labels_onehot=True)]),\n    )\n\n    # Get the end-to-end Model class\
      \ \n    model = tr.Model(head)\n\n    from transformers4rec.config.trainer import\
      \ T4RecTrainingArguments\n    from transformers4rec.torch import Trainer\n\n\
      \    #Set arguments for training \n    training_args = T4RecTrainingArguments(\n\
      \            output_dir=training_args_output_dir,\n            max_sequence_length=training_args_max_sequence_length,\n\
      \            data_loader_engine=training_args_data_loader_engine,\n        \
      \    num_train_epochs=training_args_num_train_epochs, \n            dataloader_drop_last=training_args_dataloader_drop_last,\n\
      \            per_device_train_batch_size = training_args_per_device_train_batch_size,\n\
      \            per_device_eval_batch_size = training_args_per_device_eval_batch_size,\n\
      \            gradient_accumulation_steps = training_args_gradient_accumulation_steps,\n\
      \            learning_rate=training_args_learning_rate,\n            report_to\
      \ = training_args_report_to,\n            logging_steps=training_args_logging_steps,\n\
      \    )\n\n    # Instantiate the T4Rec Trainer, which manages training and evaluation\n\
      \    trainer = Trainer(\n        model=model,\n        args=training_args,\n\
      \        schema=schema,\n        compute_metrics=True,\n    )\n\n    OUTPUT_DIR\
      \ = os.environ.get(\"OUTPUT_DIR\", Data_Output)\n\n    start_time_window_index\
      \ = 1\n    final_time_window_index = 4\n    for time_index in range(start_time_window_index,\
      \ final_time_window_index):\n        # Set data \n        time_index_train =\
      \ time_index\n        time_index_eval = time_index + 1\n        train_paths\
      \ = glob.glob(os.path.join(OUTPUT_DIR, f\"{time_index_train}/train.parquet\"\
      ))\n        eval_paths = glob.glob(os.path.join(OUTPUT_DIR, f\"{time_index_eval}/valid.parquet\"\
      ))\n        # Train on day related to time_index \n        print('*'*20)\n \
      \       print(\"Launch training for day %s are:\" %time_index)\n        print('*'*20\
      \ + '\\n')\n        trainer.train_dataset_or_path = train_paths\n        trainer.reset_lr_scheduler()\n\
      \        trainer.train()\n        trainer.state.global_step +=1\n        # Evaluate\
      \ on the following day\n        trainer.eval_dataset_or_path = eval_paths\n\
      \        train_metrics = trainer.evaluate(metric_key_prefix='eval')\n      \
      \  print('*'*20)\n        print(\"Eval results for day %s are:\\t\" %time_index_eval)\n\
      \        print('\\n' + '*'*20 + '\\n')\n        for key in sorted(train_metrics.keys()):\n\
      \            print(\" %s = %s\" % (key, str(train_metrics[key]))) \n       \
      \ wipe_memory()\n\n    # Exporting the preprocessing worflow and model for deployment\
      \ to Triton server\n    import nvtabular as nvt\n    workflow_path = os.path.join(INPUT_DATA_DIR,\
      \ 'workflow_etl')\n    print(workflow_path)\n    workflow = nvt.Workflow.load(workflow_path)\n\
      \n    # dictionary representing max sequence length for the sequential (list)\
      \ columns\n    sparse_features_max = {\n        fname: sequence_length\n   \
      \     for fname in x_cat_names + x_cont_names\n    }\n\n    sparse_features_max\n\
      \n\n    model_path=\"/dli/task/model_repository\"\n    name= \"t4r_pytorch\"\
      \n    from nvtabular.inference.triton import export_pytorch_ensemble\n    export_pytorch_ensemble(\n\
      \        model,\n        workflow,\n        sparse_max=sparse_features_max,\n\
      \        name= Output_Model_Name,\n        model_path= Output_Model_Path,\n\
      \        label_columns =[],\n    )\n\n    #implement named tuple\n    from collections\
      \ import namedtuple\n    model_info = namedtuple('Model',['Model_Path','Model_Name'])\n\
      \    return model_info(Output_Model_Path,Output_Model_Name)\n\n"
    args:
    - --executor_input
    - {executorInput: null}
    - --function_to_execute
    - session_based_recsys
