name: t4rec-kfp-e2e
metadata:
  annotations:
    sdk: 'https://cloud-pipelines.github.io/pipeline-editor/'
inputs:
  - name: Input 3
    annotations:
      editor.position: '{"x":-130,"y":100,"width":180,"height":40}'
  - name: Input
    annotations:
      editor.position: '{"x":90,"y":0,"width":180,"height":40}'
  - name: Input 2
    annotations:
      editor.position: '{"x":310,"y":100,"width":180,"height":40}'
  - name: Input 4
    annotations:
      editor.position: '{"x":310,"y":160,"width":180,"height":40}'
  - name: Input 5
    annotations:
      editor.position: '{"x":480,"y":360,"width":180,"height":40}'
  - name: Input 6
    annotations:
      editor.position: '{"x":490,"y":460,"width":180,"height":40}'
outputs: []
implementation:
  graph:
    tasks:
      Download from GCS:
        componentRef:
          url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/47f3621344c884666a926c8a15d77562f1cc5e0a/components/google-cloud/storage/download/component.yaml'
        annotations:
          editor.position: '{"x":90,"y":100,"width":180,"height":40}'
        arguments:
          GCS path:
            graphInput:
              inputName: Input
      Etl with nvtabular op:
        componentRef:
          spec:
            name: Etl with nvtabular op
            inputs:
              - name: cat_feat_list
                type: JsonArray
                default: '["user_session", "category_code", "brand", "user_id", "product_id", "category_id", "event_type"]'
                optional: true
              - name: Data_Input
                type: String
                default: /dli/task/data/
                optional: true
              - name: session_feat_list
                type: JsonArray
                default: '["event_time_ts"]'
                optional: true
            outputs:
              - name: output_path
                type: String
            implementation:
              container:
                image: 'nvcr.io/nvidia/merlin/merlin-inference:21.11'
                command:
                  - sh
                  - '-ec'
                  - |
                    program_path=$(mktemp -d)
                    printf "%s" "$0" > "$program_path/ephemeral_component.py"
                    python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
                  - |+

                    import kfp
                    from kfp.v2 import dsl
                    from kfp.v2.dsl import *
                    from typing import *

                    def etl_with_nvtabular_op(
                    # - Categorify categorical features with `Categorify()` op
                        cat_feat_list: list = ['user_session', 'category_code', 'brand', 'user_id', 'product_id', 'category_id', 'event_type'],
                        Data_Input: str="/dli/task/data/",
                        session_feat_list: list=['event_time_ts']
                    )->NamedTuple('Output',[('output_path',str)]):
                        # Import Libraries
                        import os
                        import numpy as np 
                        import cupy as cp
                        import glob
                        import cudf
                        import nvtabular as nvt
                        from nvtabular import ColumnSelector
                        from nvtabular.ops import Operator
                        from numba import config        
                        from preprocessing import etl
                        import feature_utils
                        from transformers4rec.data.preprocessing import save_time_based_splits

                        # Set up Input and Output Data Paths
                        INPUT_DATA_DIR = os.environ.get("INPUT_DATA_DIR",Data_Input)
                        # os.environ.get("INPUT_DATA_DIR", "/dli/task/data/")
                        # Read the Input Parquet file
                        df = cudf.read_parquet(os.path.join(INPUT_DATA_DIR, 'Oct-2019.parquet'))  


                        # categorify features ['user_session', 'category_code', 'brand', 'user_id', 'product_id', 'category_id', 'event_type']
                        # Initialize NVTabular Workflow

                        # Categorical Features Encoding
                        cat_feats = cat_feat_list >> nvt.ops.Categorify(start_index=1)



                        # Create temporal features with a `user-defined custom` op and `Lambda` op


                            # create time features
                            # Extract Temporal Features
                        session_ts = session_feat_list
                        session_time = (
                            session_ts >> 
                            nvt.ops.LambdaOp(lambda col: cudf.to_datetime(col, unit='s')) >> 
                            nvt.ops.Rename(name = 'event_time_dt')
                        )

                        sessiontime_weekday = (
                            session_time >> 
                            nvt.ops.LambdaOp(lambda col: col.dt.weekday) >> 
                            nvt.ops.Rename(name ='et_dayofweek')
                        )

                        def get_cycled_feature_value_sin(col, max_value):
                            value_scaled = (col + 0.000001) / max_value
                            value_sin = np.sin(2*np.pi*value_scaled)
                            return value_sin

                        def get_cycled_feature_value_cos(col, max_value):
                            value_scaled = (col + 0.000001) / max_value
                            value_cos = np.cos(2*np.pi*value_scaled)
                            return value_cos

                        weekday_sin = sessiontime_weekday >> (lambda col: get_cycled_feature_value_sin(col+1, 7)) >> nvt.ops.Rename(name = 'et_dayofweek_sin')
                        weekday_cos= sessiontime_weekday >> (lambda col: get_cycled_feature_value_cos(col+1, 7)) >> nvt.ops.Rename(name = 'et_dayofweek_cos')

                        # Add Product Recency feature

                        class ItemRecency(Operator):
                            def transform(self, columns, gdf):
                                for column in columns.names:
                                    col = gdf[column]
                                    item_first_timestamp = gdf['prod_first_event_time_ts']
                                    delta_days = (col - item_first_timestamp) / (60*60*24)
                                    gdf[column + "_age_days"] = delta_days * (delta_days >=0)
                                return gdf

                            def output_column_names(self, columns):
                                return ColumnSelector([column + "_age_days" for column in columns.names])

                            def dependencies(self):
                                return ["prod_first_event_time_ts"]


                        recency_features = ['event_time_ts'] >> ItemRecency() 
                        recency_features_norm = recency_features >> nvt.ops.LogOp() >> nvt.ops.Normalize() >> nvt.ops.Rename(name='product_recency_days_log_norm')

                        time_features = (
                            session_time +
                            sessiontime_weekday +
                            weekday_sin +
                            weekday_cos +
                            recency_features_norm
                        )


                        # - Transform continuous features using `Log` and `Normalize` ops


                            # Smoothing price long-tailed distribution and applying standardization
                        price_log = ['price'] >> nvt.ops.LogOp() >> nvt.ops.Normalize() >> nvt.ops.Rename(name='price_log_norm')
                        # Normalize Continuous FeaturesÂ¶
                        # Relative price to the average price for the category_id
                        def relative_price_to_avg_categ(col, gdf):
                            epsilon = 1e-5
                            col = ((gdf['price'] - col) / (col + epsilon)) * (col > 0).astype(int)
                            return col

                        avg_category_id_pr = ['category_id'] >> nvt.ops.JoinGroupby(cont_cols =['price'], stats=["mean"]) >> nvt.ops.Rename(name='avg_category_id_price')
                        relative_price_to_avg_category = avg_category_id_pr >> nvt.ops.LambdaOp(relative_price_to_avg_categ, dependency=['price']) >> nvt.ops.Rename(name="relative_price_to_avg_categ_id")


                        # - Group all these features together at the session level sorting the interactions by time with `Groupby`



                        groupby_feats = ['event_time_ts', 'user_session'] + cat_feats + time_features + price_log + relative_price_to_avg_category

                            # Define Groupby Workflow
                        groupby_features = groupby_feats >> nvt.ops.Groupby(
                            groupby_cols=["user_session"], 
                            sort_cols=["event_time_ts"],
                            aggs={
                                'user_id': ['first'],
                                'product_id': ["list", "count"],
                                'category_code': ["list"],  
                                'event_type': ["list"], 
                                'brand': ["list"], 
                                'category_id': ["list"], 
                                'event_time_ts': ["first"],
                                'event_time_dt': ["first"],
                                'et_dayofweek_sin': ["list"],
                                'et_dayofweek_cos': ["list"],
                                'price_log_norm': ["list"],
                                'relative_price_to_avg_categ_id': ["list"],
                                'product_recency_days_log_norm': ["list"]
                                },
                            name_sep="-")

                        groupby_features_list = groupby_features['product_id-list',
                            'category_code-list',  
                            'event_type-list', 
                            'brand-list', 
                            'category_id-list', 
                            'et_dayofweek_sin-list',
                            'et_dayofweek_cos-list',
                            'price_log_norm-list',
                            'relative_price_to_avg_categ_id-list',
                            'product_recency_days_log_norm-list']


                        SESSIONS_MAX_LENGTH = 20 
                        MINIMUM_SESSION_LENGTH = 2
                        # Grouping interactions into sessions
                        # Aggregate by session id and creates the sequential features
                        groupby_features_trim = groupby_features_list >> nvt.ops.ListSlice(0,SESSIONS_MAX_LENGTH) >> nvt.ops.Rename(postfix = '_seq')

                        # calculate session day index based on 'timestamp-first' column
                        day_index = ((groupby_features['event_time_dt-first'])  >> 
                            nvt.ops.LambdaOp(lambda col: (col - col.min()).dt.days +1) >> 
                            nvt.ops.Rename(f = lambda col: "day_index")
                        )

                        selected_features = groupby_features['user_session', 'product_id-count'] + groupby_features_trim + day_index

                        filtered_sessions = selected_features >> nvt.ops.Filter(f=lambda df: df["product_id-count"] >= MINIMUM_SESSION_LENGTH)

                        # avoid numba warnings

                        config.CUDA_LOW_OCCUPANCY_WARNINGS = 0

                        dataset = nvt.Dataset(df)
                        workflow = nvt.Workflow(filtered_sessions)
                        workflow.fit(dataset)
                        sessions_gdf = workflow.transform(dataset).to_ddf()

                        workflow_path = os.path.join(INPUT_DATA_DIR, 'workflow_etl')
                        workflow.save(workflow_path)





                            # define partition column
                        PARTITION_COL = 'day_index'
                        # make changes here use gcs upload component
                        # define output_folder to store the partitioned parquet files
                        OUTPUT_FOLDER = os.environ.get("OUTPUT_FOLDER", INPUT_DATA_DIR + "sessions_by_day")
                        import subprocess
                        subprocess.run(["mkdir", "-p",OUTPUT_FOLDER])

                        save_time_based_splits(data=nvt.Dataset(sessions_gdf),
                                           output_dir= OUTPUT_FOLDER,
                                           partition_col=PARTITION_COL,
                                           timestamp_col='user_session', 
                                          )
                        #implement named tuple
                        from collections import namedtuple
                        upload_file_path = namedtuple('Output',['OutputPath_Name'])
                        return upload_file_path(OUTPUT_FOLDER)

                args:
                  - '--executor_input'
                  - executorInput: null
                  - '--function_to_execute'
                  - etl_with_nvtabular_op
          digest: 821cd1a3e34e6f40dd8321d0253273539c024abd5a541baf956fb1561609ef4b
        annotations:
          editor.position: '{"x":90,"y":180,"width":180,"height":40}'
        arguments:
          cat_feat_list:
            graphInput:
              inputName: Input 3
          session_feat_list:
            graphInput:
              inputName: Input 2
          Data_Input:
            taskOutput:
              taskId: Download from GCS
              outputName: Data
      Upload to GCS:
        componentRef:
          url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/47f3621344c884666a926c8a15d77562f1cc5e0a/components/google-cloud/storage/upload_to_explicit_uri/component.yaml'
        annotations:
          editor.position: '{"x":270,"y":280,"width":180,"height":40}'
        arguments:
          GCS path:
            graphInput:
              inputName: Input 4
          Data:
            taskOutput:
              taskId: Etl with nvtabular op
              outputName: output_path
      Session based recsys:
        componentRef:
          spec:
            name: Session based recsys
            inputs:
              - name: Output_Model_Path
                type: String
                default: /dli/task/model_repository
                optional: true
              - name: Output_Model_Name
                type: String
                default: t4r_pytorch
                optional: true
              - name: Data_Input
                type: String
                default: /dli/task/data/
                optional: true
              - name: Data_Output
                type: String
                default: /dli/task/data/sessions_by_day
                optional: true
              - name: local_rank
                type: Integer
                default: '-1'
                optional: true
              - name: training_args_output_dir
                type: String
                default: ./tmp
                optional: true
              - name: training_args_max_sequence_length
                type: Integer
                default: '20'
                optional: true
              - name: training_args_data_loader_engine
                type: String
                default: nvtabular
                optional: true
              - name: training_args_num_train_epochs
                type: Integer
                default: '3'
                optional: true
              - name: training_args_dataloader_drop_last
                default: 'False'
                optional: true
              - name: training_args_per_device_train_batch_size
                type: Integer
                default: '256'
                optional: true
              - name: training_args_per_device_eval_batch_size
                type: Integer
                default: '32'
                optional: true
              - name: training_args_gradient_accumulation_steps
                type: Integer
                default: '1'
                optional: true
              - name: training_args_learning_rate
                type: Integer
                optional: true
              - name: training_args_report_to
                type: JsonArray
                default: '[]'
                optional: true
              - name: training_args_logging_steps
                type: Integer
                default: '200'
                optional: true
            outputs:
              - name: model
                type: String
            implementation:
              container:
                image: 'nvcr.io/nvidia/merlin/merlin-pytorch-training:21.11'
                command:
                  - sh
                  - '-ec'
                  - |
                    program_path=$(mktemp -d)
                    printf "%s" "$0" > "$program_path/ephemeral_component.py"
                    python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
                  - |+

                    import kfp
                    from kfp.v2 import dsl
                    from kfp.v2.dsl import *
                    from typing import *

                    def session_based_recsys(    
                        Output_Model_Path: str="/dli/task/model_repository",
                        Output_Model_Name: str = "t4r_pytorch",
                        Data_Input: str="/dli/task/data/",
                        Data_Output: str = "/dli/task/data/sessions_by_day",
                        local_rank : int= -1,
                        training_args_output_dir: str="./tmp",
                        training_args_max_sequence_length: int=20,
                        training_args_data_loader_engine: str='nvtabular',
                        training_args_num_train_epochs: int=3, 
                        training_args_dataloader_drop_last=False,
                        training_args_per_device_train_batch_size : int= 256,
                        training_args_per_device_eval_batch_size : int= 32,
                        training_args_gradient_accumulation_steps : int= 1,
                        training_args_learning_rate: int=0.000666,
                        training_args_report_to :list= [],
                        training_args_logging_steps: int=200,
                    )->NamedTuple('Model',[('model',str)]):
                        # Training an RNN-based Session-based Recommendation Model
                        import os
                        import glob

                        import torch 
                        import transformers4rec.torch as tr

                        from transformers4rec.torch.ranking_metric import NDCGAt, RecallAt
                        from transformers4rec.torch.utils.examples_utils import wipe_memory

                        # Instantiates Schema object from a `schema` file.

                        from merlin_standard_lib import Schema
                        # Define schema object to pass it to the TabularSequenceFeatures class
                        INPUT_DATA_DIR = Data_Input

                        SCHEMA_PATH = os.path.join(INPUT_DATA_DIR, 'schema_tutorial.pb')
                        schema = Schema().from_proto_text(SCHEMA_PATH)
                        schema = schema.select_by_name(['product_id-list_seq'])

                        # Defining the input block: `TabularSequenceFeatures`

                        # Define input block
                        sequence_length = 20
                        inputs = tr.TabularSequenceFeatures.from_schema(
                                schema,
                                max_sequence_length= sequence_length,
                                masking = 'causal',
                            )

                        # Connecting the blocks with `SequentialBlock`

                        d_model = 128
                        body = tr.SequentialBlock(
                                inputs,
                                tr.MLPBlock([d_model]),
                                tr.Block(torch.nn.GRU(input_size=d_model, hidden_size=d_model, num_layers=1), [None, 20, d_model])
                        )

                        # Item Prediction head and tying embeddings

                        head = tr.Head(
                        body,
                            tr.NextItemPredictionTask(weight_tying=True, hf_format=True, 
                                                    metrics=[NDCGAt(top_ks=[10, 20], labels_onehot=True),  
                                                            RecallAt(top_ks=[10, 20], labels_onehot=True)]),
                        )
                        model = tr.Model(head)

                        # Define a Dataloader function from schema

                        # import NVTabular dependencies
                        from transformers4rec.torch.utils.data_utils import NVTabularDataLoader

                        x_cat_names, x_cont_names = ['product_id-list_seq'], []

                        # dictionary representing max sequence length for column
                        sparse_features_max = {
                            fname: sequence_length
                            for fname in x_cat_names + x_cont_names
                        }

                        # Define a `get_dataloader` function to call in the training loop
                        def get_dataloader(path, batch_size=32):

                            return NVTabularDataLoader.from_schema(
                                schema,
                                path, 
                                batch_size,
                                max_sequence_length=sequence_length,
                                sparse_names=x_cat_names + x_cont_names,
                                sparse_max=sparse_features_max,
                        )

                        # Daily Fine-Tuning: Training over a time window

                        from transformers4rec.config.trainer import T4RecTrainingArguments
                        from transformers4rec.torch import Trainer

                        #Set arguments for training 
                        train_args = T4RecTrainingArguments(local_rank = local_rank, 
                                                            dataloader_drop_last = training_args_dataloader_drop_last,
                                                            report_to = training_args_report_to,   #set empy list to avoig logging metrics to Weights&Biases
                                                            gradient_accumulation_steps = training_args_gradient_accumulation_steps,
                                                            per_device_train_batch_size = training_args_per_device_train_batch_size, 
                                                            per_device_eval_batch_size = training_args_per_device_eval_batch_size,
                                                            output_dir = training_args_output_dir, 
                                                            max_sequence_length=sequence_length,
                                                            learning_rate=0.00071,
                                                            num_train_epochs=training_args_num_train_epochs,
                                                            logging_steps=training_args_logging_steps,
                                                        )

                        # Instantiate the T4Rec Trainer, which manages training and evaluation
                        trainer = Trainer(
                            model=model,
                            args=train_args,
                            schema=schema,
                            compute_metrics=True,
                        )
                        #recieve input from the 
                        OUTPUT_DIR = os.environ.get("OUTPUT_DIR", Data_Output)

                        # os.system("%%time")

                        start_time_window_index = 1
                        final_time_window_index = 4
                        for time_index in range(start_time_window_index, final_time_window_index):
                            # Set data 
                            time_index_train = time_index
                            time_index_eval = time_index + 1
                            train_paths = glob.glob(os.path.join(OUTPUT_DIR, f"{time_index_train}/train.parquet"))
                            eval_paths = glob.glob(os.path.join(OUTPUT_DIR, f"{time_index_eval}/valid.parquet"))
                            # Train on day related to time_index 
                            print('*'*20)
                            print("Launch training for day %s are:" %time_index)
                            print('*'*20 + '\n')
                            trainer.train_dataset_or_path = train_paths
                            trainer.reset_lr_scheduler()
                            trainer.train()
                            trainer.state.global_step +=1
                            # Evaluate on the following day
                            trainer.eval_dataset_or_path = eval_paths
                            train_metrics = trainer.evaluate(metric_key_prefix='eval')
                            print('*'*20)
                            print("Eval results for day %s are:\t" %time_index_eval)
                            print('\n' + '*'*20 + '\n')
                            for key in sorted(train_metrics.keys()):
                                print(" %s = %s" % (key, str(train_metrics[key]))) 
                            wipe_memory()

                            from collections import namedtuple
                            upload_file_path = namedtuple('Output',['OutputPath_Name'])
                            return upload_file_path(model)


                        # Training a Transformer-based Session-based Recommendation Model

                        import os
                        import glob

                        import torch 
                        import transformers4rec.torch as tr

                        from transformers4rec.torch.ranking_metric import NDCGAt, RecallAt
                        from transformers4rec.torch.utils.examples_utils import wipe_memory

                        # As we did above, we start with defining our schema object and filtering only the `product_id` feature for training.
                        from merlin_standard_lib import Schema
                        # Define schema object to pass it to the TabularSequenceFeatures class
                        INPUT_DATA_DIR = Data_Input

                        SCHEMA_PATH = os.path.join(INPUT_DATA_DIR, 'schema_tutorial.pb')
                        schema = Schema().from_proto_text(SCHEMA_PATH)
                        schema = schema.select_by_name(['product_id-list_seq'])

                        # Define input block
                        sequence_length, d_model = 20, 192
                        # Define input module to process tabular input-features and to prepare masked inputs
                        inputs= tr.TabularSequenceFeatures.from_schema(
                            schema,
                            max_sequence_length=sequence_length,
                            d_output=d_model,
                            masking="mlm",
                        )

                        # Define XLNetConfig class and set default parameters for HF XLNet config  
                        transformer_config = tr.XLNetConfig.build(
                            d_model=d_model, n_head=4, n_layer=2, total_seq_length=sequence_length
                        )
                        # Define the model block including: inputs, masking, projection and transformer block.
                        body = tr.SequentialBlock(
                            inputs, tr.MLPBlock([192]), tr.TransformerBlock(transformer_config, masking=inputs.masking)
                        )

                        # Define the head for to next item prediction task 
                        head = tr.Head(
                            body,
                            tr.NextItemPredictionTask(weight_tying=True, hf_format=True, 
                                                    metrics=[NDCGAt(top_ks=[10, 20], labels_onehot=True),  
                                                            RecallAt(top_ks=[10, 20], labels_onehot=True)]),
                        )

                        # Get the end-to-end Model class 
                        model = tr.Model(head)

                        from transformers4rec.config.trainer import T4RecTrainingArguments
                        from transformers4rec.torch import Trainer

                        #Set arguments for training 
                        training_args = T4RecTrainingArguments(
                                    output_dir=training_args_output_dir,
                                    max_sequence_length=training_args_max_sequence_length,
                                    data_loader_engine=training_args_data_loader_engine,
                                    num_train_epochs=training_args_num_train_epochs, 
                                    dataloader_drop_last=training_args_dataloader_drop_last,
                                    per_device_train_batch_size = training_args_per_device_train_batch_size,
                                    per_device_eval_batch_size = training_args_per_device_eval_batch_size,
                                    gradient_accumulation_steps = training_args_gradient_accumulation_steps,
                                    learning_rate=training_args_learning_rate,
                                    report_to = training_args_report_to,
                                    logging_steps=training_args_logging_steps,
                                )

                        # Instantiate the T4Rec Trainer, which manages training and evaluation
                        trainer = Trainer(
                            model=model,
                            args=training_args,
                            schema=schema,
                            compute_metrics=True,
                        )

                        OUTPUT_DIR = os.environ.get("OUTPUT_DIR", Data_Output)

                        # os.system("%%time")
                        start_time_window_index = 1
                        final_time_window_index = 4
                        for time_index in range(start_time_window_index, final_time_window_index):
                            # Set data 
                            time_index_train = time_index
                            time_index_eval = time_index + 1
                            train_paths = glob.glob(os.path.join(OUTPUT_DIR, f"{time_index_train}/train.parquet"))
                            eval_paths = glob.glob(os.path.join(OUTPUT_DIR, f"{time_index_eval}/valid.parquet"))
                            # Train on day related to time_index 
                            print('*'*20)
                            print("Launch training for day %s are:" %time_index)
                            print('*'*20 + '\n')
                            trainer.train_dataset_or_path = train_paths
                            trainer.reset_lr_scheduler()
                            trainer.train()
                            trainer.state.global_step +=1
                            # Evaluate on the following day
                            trainer.eval_dataset_or_path = eval_paths
                            train_metrics = trainer.evaluate(metric_key_prefix='eval')
                            print('*'*20)
                            print("Eval results for day %s are:\t" %time_index_eval)
                            print('\n' + '*'*20 + '\n')
                            for key in sorted(train_metrics.keys()):
                                print(" %s = %s" % (key, str(train_metrics[key]))) 
                            wipe_memory()

                            # Train XLNET with Side Information for Next Item Prediction

                        import os
                        import glob
                        import nvtabular as nvt

                        import torch 
                        import transformers4rec.torch as tr

                        from transformers4rec.torch.ranking_metric import NDCGAt, RecallAt
                        from transformers4rec.torch.utils.examples_utils import wipe_memory

                        # Define categorical and continuous columns to fed to training model 
                        # add it in parameters
                        x_cat_names = ['product_id-list_seq', 'category_id-list_seq', 'brand-list_seq']
                        x_cont_names = ['product_recency_days_log_norm-list_seq', 'et_dayofweek_sin-list_seq', 'et_dayofweek_cos-list_seq', 
                                        'price_log_norm-list_seq', 'relative_price_to_avg_categ_id-list_seq']


                        from merlin_standard_lib import Schema

                        # Define schema object to pass it to the TabularSequenceFeatures class
                        INPUT_DATA_DIR = os.environ.get("INPUT_DATA_DIR", "/dli/task/data/")

                        SCHEMA_PATH = os.path.join(INPUT_DATA_DIR, 'schema_tutorial.pb')
                        schema = Schema().from_proto_text(SCHEMA_PATH)
                        schema = schema.select_by_name(x_cat_names + x_cont_names)

                        # Define input block
                        sequence_length, d_model = 20, 192
                        # Define input module to process tabular input-features and to prepare masked inputs
                        inputs= tr.TabularSequenceFeatures.from_schema(
                            schema,
                            max_sequence_length=sequence_length,
                            aggregation="concat",
                            d_output=d_model,
                            masking="mlm",
                        )

                        # Define XLNetConfig class and set default parameters for HF XLNet config  
                        transformer_config = tr.XLNetConfig.build(
                            d_model=d_model, n_head=4, n_layer=2, total_seq_length=sequence_length
                        )
                        # Define the model block including: inputs, masking, projection and transformer block.
                        body = tr.SequentialBlock(
                            inputs, tr.MLPBlock([192]), tr.TransformerBlock(transformer_config, masking=inputs.masking)
                        )

                        # Define the head related to next item prediction task 
                        head = tr.Head(
                            body,
                            tr.NextItemPredictionTask(weight_tying=True, hf_format=True, 
                                                            metrics=[NDCGAt(top_ks=[10, 20], labels_onehot=True),  
                                                                    RecallAt(top_ks=[10, 20], labels_onehot=True)]),
                        )

                        # Get the end-to-end Model class 
                        model = tr.Model(head)

                        from transformers4rec.config.trainer import T4RecTrainingArguments
                        from transformers4rec.torch import Trainer

                        #Set arguments for training 
                        training_args = T4RecTrainingArguments(
                                output_dir=training_args_output_dir,
                                max_sequence_length=training_args_max_sequence_length,
                                data_loader_engine=training_args_data_loader_engine,
                                num_train_epochs=training_args_num_train_epochs, 
                                dataloader_drop_last=training_args_dataloader_drop_last,
                                per_device_train_batch_size = training_args_per_device_train_batch_size,
                                per_device_eval_batch_size = training_args_per_device_eval_batch_size,
                                gradient_accumulation_steps = training_args_gradient_accumulation_steps,
                                learning_rate=training_args_learning_rate,
                                report_to = training_args_report_to,
                                logging_steps=training_args_logging_steps,
                        )

                        # Instantiate the T4Rec Trainer, which manages training and evaluation
                        trainer = Trainer(
                            model=model,
                            args=training_args,
                            schema=schema,
                            compute_metrics=True,
                        )

                        OUTPUT_DIR = os.environ.get("OUTPUT_DIR", Data_Output)

                        start_time_window_index = 1
                        final_time_window_index = 4
                        for time_index in range(start_time_window_index, final_time_window_index):
                            # Set data 
                            time_index_train = time_index
                            time_index_eval = time_index + 1
                            train_paths = glob.glob(os.path.join(OUTPUT_DIR, f"{time_index_train}/train.parquet"))
                            eval_paths = glob.glob(os.path.join(OUTPUT_DIR, f"{time_index_eval}/valid.parquet"))
                            # Train on day related to time_index 
                            print('*'*20)
                            print("Launch training for day %s are:" %time_index)
                            print('*'*20 + '\n')
                            trainer.train_dataset_or_path = train_paths
                            trainer.reset_lr_scheduler()
                            trainer.train()
                            trainer.state.global_step +=1
                            # Evaluate on the following day
                            trainer.eval_dataset_or_path = eval_paths
                            train_metrics = trainer.evaluate(metric_key_prefix='eval')
                            print('*'*20)
                            print("Eval results for day %s are:\t" %time_index_eval)
                            print('\n' + '*'*20 + '\n')
                            for key in sorted(train_metrics.keys()):
                                print(" %s = %s" % (key, str(train_metrics[key]))) 
                            wipe_memory()

                        # Exporting the preprocessing worflow and model for deployment to Triton server
                        import nvtabular as nvt
                        workflow_path = os.path.join(INPUT_DATA_DIR, 'workflow_etl')
                        print(workflow_path)
                        workflow = nvt.Workflow.load(workflow_path)

                        # dictionary representing max sequence length for the sequential (list) columns
                        sparse_features_max = {
                            fname: sequence_length
                            for fname in x_cat_names + x_cont_names
                        }

                        sparse_features_max


                        model_path="/dli/task/model_repository"
                        name= "t4r_pytorch"
                        from nvtabular.inference.triton import export_pytorch_ensemble
                        export_pytorch_ensemble(
                            model,
                            workflow,
                            sparse_max=sparse_features_max,
                            name= Output_Model_Name,
                            model_path= Output_Model_Path,
                            label_columns =[],
                        )

                        #implement named tuple
                        from collections import namedtuple
                        model_info = namedtuple('Model',['Model_Path','Model_Name'])
                        return model_info(Output_Model_Path,Output_Model_Name)

                args:
                  - '--executor_input'
                  - executorInput: null
                  - '--function_to_execute'
                  - session_based_recsys
          digest: 6aa96e415b4f1332e55040580825e179ffdeeea67f8bc974c898d61a0484078d
        annotations:
          editor.position: '{"x":-10,"y":360,"width":180,"height":40}'
        arguments:
          Data_Input:
            taskOutput:
              taskId: Etl with nvtabular op
              outputName: output_path
      Upload to GCS 2:
        componentRef:
          url: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/47f3621344c884666a926c8a15d77562f1cc5e0a/components/google-cloud/storage/upload_to_explicit_uri/component.yaml'
        annotations:
          editor.position: '{"x":280,"y":460,"width":180,"height":40}'
        arguments:
          Data:
            taskOutput:
              taskId: Session based recsys
              outputName: model
          GCS path:
            graphInput:
              inputName: Input 5
      Serve a model with KServe:
        componentRef:
          digest: 307a23ef97a6445560fffb87dc31cf3f8d146400a4af03613999e3cdd2f905e9
          url: 'https://raw.githubusercontent.com/markwinter/pipelines/kserve-component/components/kserve/component.yaml'
        annotations:
          editor.position: '{"x":60,"y":620,"width":180,"height":40}'
        arguments:
          Model URI:
            taskOutput:
              taskId: Upload to GCS 2
              outputName: GCS path
          Model Name:
            taskOutput:
              taskId: Session based recsys
              outputName: model
          Framework:
            graphInput:
              inputName: Input 6
    outputValues: {}
